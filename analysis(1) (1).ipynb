{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3551217",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis - Python Take-Home Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0efa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This assessment involves analyzing the Calgary HTTP dataset, which contains approximately one year's worth of HTTP requests to the University of Calgary's Computer Science web server. You'll work with real-world web server log data to extract meaningful insights and demonstrate your Python data analysis skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81debeba",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e728b83",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Work in the cells below - You can add as many cells as needed for data loading, cleaning, and exploration\n",
    "* Import required libraries\n",
    "* Implement data loading and cleaning - Create functions to download, parse, and clean the log data\n",
    "* Explore the data - Understand the structure and identify any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92bc0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines loaded: 726739\n",
      "    host rfc931 authuser                   datetime                  request  \\\n",
      "0  local      -        -  1994-10-24 13:41:41-06:00  GET index.html HTTP/1.0   \n",
      "1  local      -        -  1994-10-24 13:41:41-06:00       GET 1.gif HTTP/1.0   \n",
      "2  local      -        -  1994-10-24 13:43:13-06:00  GET index.html HTTP/1.0   \n",
      "3  local      -        -  1994-10-24 13:43:14-06:00       GET 2.gif HTTP/1.0   \n",
      "4  local      -        -  1994-10-24 13:43:15-06:00       GET 3.gif HTTP/1.0   \n",
      "\n",
      "   status  bytes method    filename  protocol extension  \n",
      "0     200    150    GET  index.html  HTTP/1.0      html  \n",
      "1     200   1210    GET       1.gif  HTTP/1.0       gif  \n",
      "2     200   3185    GET  index.html  HTTP/1.0      html  \n",
      "3     200   2555    GET       2.gif  HTTP/1.0       gif  \n",
      "4     200  36403    GET       3.gif  HTTP/1.0       gif  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 724836 entries, 0 to 724835\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   host       724836 non-null  object\n",
      " 1   rfc931     724836 non-null  object\n",
      " 2   authuser   724836 non-null  object\n",
      " 3   datetime   724836 non-null  object\n",
      " 4   request    724836 non-null  object\n",
      " 5   status     724836 non-null  int64 \n",
      " 6   bytes      724836 non-null  int64 \n",
      " 7   method     724836 non-null  object\n",
      " 8   filename   724836 non-null  object\n",
      " 9   protocol   724836 non-null  object\n",
      " 10  extension  724836 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 60.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Parsing the Full Log File into a Clean DataFrame\n",
    "\n",
    "import gzip\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "file_path = r\"C:\\Users\\Arshad\\Downloads\\calgary_access_log.gz\"\n",
    "\n",
    "# Read and parse the file\n",
    "log_entries = []\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        log_entries.append(line.strip())\n",
    "print(f\"Total lines loaded: {len(log_entries)}\")\n",
    "\n",
    "# Define the log parsing function\n",
    "log_pattern = re.compile(\n",
    "    r'^(?P<host>\\S+) (?P<rfc931>\\S+) (?P<authuser>\\S+) \\[(?P<datetime>[^\\]]+)\\] '\n",
    "    r'\"(?P<request>[^\"]*)\" (?P<status>\\d{3}) (?P<bytes>\\S+)$'\n",
    ")\n",
    "\n",
    "def parse_log_line(line):\n",
    "    match = log_pattern.match(line)\n",
    "    if not match:\n",
    "        return None\n",
    "    data = match.groupdict()\n",
    "    try:\n",
    "        data['datetime'] = datetime.strptime(data['datetime'], \"%d/%b/%Y:%H:%M:%S %z\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "    data['method'], data['filename'], data['protocol'] = ('', '', '')\n",
    "    if data['request']:\n",
    "        parts = data['request'].split()\n",
    "        if len(parts) == 3:\n",
    "            data['method'], data['filename'], data['protocol'] = parts\n",
    "    try:\n",
    "        data['status'] = int(data['status'])\n",
    "    except:\n",
    "        data['status'] = None\n",
    "    data['bytes'] = int(data['bytes']) if data['bytes'].isdigit() else 0\n",
    "    data['extension'] = data['filename'].split('.')[-1] if '.' in data['filename'] else ''\n",
    "    return data\n",
    "\n",
    "parsed_data = [parse_log_line(line) for line in log_entries]\n",
    "cleaned_data = [entry for entry in parsed_data if entry]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e491a4e",
   "metadata": {},
   "source": [
    "## ⚠️ IMPORTANT: Template Questions Section\n",
    "**DO NOT MODIFY THE TEMPLATE BELOW THIS POINT**\n",
    "\n",
    "The following section contains the assessment questions. You may add cells above this section for data loading, cleaning, and exploration, but do not modify the function signatures or structure of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da5c6e",
   "metadata": {},
   "source": [
    "## Part 2: Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1ce65",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Implement each function according to its docstring specifications\n",
    "* Use the cleaned data you prepared in Part 1\n",
    "* Ensure your functions return the exact data types specified\n",
    "* Test your functions to verify they work correctly\n",
    "* You may add helper functions, but keep the main function signatures unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff13fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q1: Count of total log records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6264dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1:\n",
      "724836\n"
     ]
    }
   ],
   "source": [
    "def total_log_records() -> int:\n",
    "    \"\"\"\n",
    "    Q1: Count of total log records.\n",
    "\n",
    "    Objective:\n",
    "        Determine the total number of HTTP log entries in the dataset.\n",
    "        Each line in the log file represents one HTTP request.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of log entries.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count log records\n",
    "\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "answer1 = total_log_records()\n",
    "print(\"Answer 1:\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5141e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q2: Count of unique hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcbccae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def unique_host_count() -> int:\n",
    "    \"\"\"\n",
    "    Q2: Count of unique hosts.\n",
    "\n",
    "    Objective:\n",
    "        Determine how many distinct hosts accessed the server.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of unique hosts.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count unique hosts\n",
    "\n",
    "    return df['host'].nunique()\n",
    "\n",
    "\n",
    "answer2 = unique_host_count()\n",
    "print(\"Answer 2:\")\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c224d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Q3: Date-wise unique filename counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac11c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3:\n",
      "{'01-Aug-1995': 670, '01-Jul-1995': 388, '01-Jun-1995': 591, '01-May-1995': 467, '01-Oct-1995': 553, '01-Sep-1995': 329, '02-Apr-1995': 439, '02-Aug-1995': 856, '02-Jul-1995': 398, '02-Jun-1995': 514, '02-May-1995': 702, '02-Oct-1995': 872, '02-Sep-1995': 350, '03-Apr-1995': 796, '03-Aug-1995': 583, '03-Jul-1995': 434, '03-Jun-1995': 399, '03-May-1995': 590, '03-Oct-1995': 847, '03-Sep-1995': 213, '04-Apr-1995': 822, '04-Aug-1995': 716, '04-Jul-1995': 611, '04-Jun-1995': 354, '04-May-1995': 685, '04-Oct-1995': 890, '04-Sep-1995': 341, '05-Apr-1995': 891, '05-Aug-1995': 508, '05-Jul-1995': 608, '05-Jun-1995': 495, '05-May-1995': 609, '05-Oct-1995': 847, '05-Sep-1995': 412, '06-Apr-1995': 679, '06-Aug-1995': 449, '06-Jul-1995': 523, '06-Jun-1995': 663, '06-May-1995': 517, '06-Oct-1995': 869, '06-Sep-1995': 550, '07-Apr-1995': 777, '07-Aug-1995': 609, '07-Jul-1995': 428, '07-Jun-1995': 486, '07-May-1995': 726, '07-Oct-1995': 468, '07-Sep-1995': 591, '08-Apr-1995': 543, '08-Aug-1995': 655, '08-Jul-1995': 277, '08-Jun-1995': 643, '08-May-1995': 657, '08-Oct-1995': 515, '08-Sep-1995': 755, '09-Apr-1995': 627, '09-Aug-1995': 699, '09-Jul-1995': 233, '09-Jun-1995': 468, '09-May-1995': 775, '09-Oct-1995': 743, '09-Sep-1995': 409, '10-Apr-1995': 751, '10-Aug-1995': 636, '10-Jul-1995': 503, '10-Jun-1995': 328, '10-May-1995': 795, '10-Oct-1995': 842, '10-Sep-1995': 456, '11-Apr-1995': 817, '11-Aug-1995': 452, '11-Jul-1995': 572, '11-Jun-1995': 298, '11-May-1995': 600, '11-Oct-1995': 718, '11-Sep-1995': 718, '12-Apr-1995': 888, '12-Aug-1995': 341, '12-Jul-1995': 468, '12-Jun-1995': 520, '12-May-1995': 469, '12-Sep-1995': 719, '13-Apr-1995': 614, '13-Aug-1995': 464, '13-Jul-1995': 500, '13-Jun-1995': 466, '13-May-1995': 289, '13-Sep-1995': 774, '14-Apr-1995': 354, '14-Aug-1995': 590, '14-Jul-1995': 552, '14-Jun-1995': 590, '14-May-1995': 326, '14-Sep-1995': 719, '15-Apr-1995': 419, '15-Aug-1995': 482, '15-Jul-1995': 385, '15-Jun-1995': 480, '15-May-1995': 584, '15-Sep-1995': 710, '16-Apr-1995': 435, '16-Aug-1995': 602, '16-Jul-1995': 300, '16-Jun-1995': 530, '16-May-1995': 432, '16-Sep-1995': 565, '17-Apr-1995': 446, '17-Aug-1995': 538, '17-Jul-1995': 568, '17-Jun-1995': 384, '17-May-1995': 509, '17-Sep-1995': 467, '18-Apr-1995': 453, '18-Aug-1995': 493, '18-Jul-1995': 558, '18-Jun-1995': 359, '18-May-1995': 529, '18-Sep-1995': 658, '19-Apr-1995': 701, '19-Aug-1995': 378, '19-Jul-1995': 472, '19-Jun-1995': 613, '19-May-1995': 499, '19-Sep-1995': 736, '20-Apr-1995': 588, '20-Aug-1995': 396, '20-Jul-1995': 570, '20-Jun-1995': 531, '20-May-1995': 254, '20-Sep-1995': 833, '21-Apr-1995': 713, '21-Aug-1995': 632, '21-Jul-1995': 650, '21-Jun-1995': 625, '21-May-1995': 289, '21-Sep-1995': 801, '22-Apr-1995': 435, '22-Aug-1995': 539, '22-Jul-1995': 445, '22-Jun-1995': 631, '22-May-1995': 478, '22-Sep-1995': 616, '23-Apr-1995': 333, '23-Aug-1995': 661, '23-Jul-1995': 499, '23-Jun-1995': 562, '23-May-1995': 566, '23-Sep-1995': 503, '24-Apr-1995': 529, '24-Aug-1995': 579, '24-Jul-1995': 566, '24-Jun-1995': 397, '24-May-1995': 490, '24-Oct-1994': 229, '24-Sep-1995': 594, '25-Apr-1995': 558, '25-Aug-1995': 596, '25-Jul-1995': 589, '25-Jun-1995': 570, '25-May-1995': 488, '25-Oct-1994': 319, '25-Sep-1995': 724, '26-Apr-1995': 648, '26-Aug-1995': 395, '26-Jul-1995': 599, '26-Jun-1995': 638, '26-May-1995': 425, '26-Oct-1994': 377, '26-Sep-1995': 871, '27-Apr-1995': 616, '27-Aug-1995': 437, '27-Jul-1995': 615, '27-Jun-1995': 518, '27-May-1995': 245, '27-Oct-1994': 385, '27-Sep-1995': 827, '28-Apr-1995': 637, '28-Aug-1995': 549, '28-Jul-1995': 565, '28-Jun-1995': 573, '28-May-1995': 205, '28-Oct-1994': 400, '28-Sep-1995': 868, '29-Apr-1995': 449, '29-Aug-1995': 512, '29-Jul-1995': 321, '29-Jun-1995': 470, '29-May-1995': 465, '29-Oct-1994': 255, '29-Sep-1995': 839, '30-Apr-1995': 278, '30-Aug-1995': 594, '30-Jul-1995': 482, '30-Jun-1995': 461, '30-May-1995': 554, '30-Oct-1994': 20, '30-Sep-1995': 651, '31-Aug-1995': 510, '31-Jul-1995': 623, '31-May-1995': 572}\n"
     ]
    }
   ],
   "source": [
    "def datewise_unique_filename_counts() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q3: Date-wise unique filename counts.\n",
    "\n",
    "    Objective:\n",
    "        For each date, count the number of unique filenames that accessed the server.\n",
    "        The date should be in 'dd-MMM-yyyy' format (e.g., '01-Jul-1995').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date to its count of unique filenames.\n",
    "              Example: {'01-Jul-1995': 123, '02-Jul-1995': 150}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic for date-wise unique filename counts\n",
    "\n",
    " # Ensure datetime column is in datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid datetime\n",
    "    df_valid = df.dropna(subset=['datetime']).copy()\n",
    "\n",
    "    # Use .loc to avoid SettingWithCopyWarning\n",
    "    df_valid.loc[:, 'date_str'] = df_valid['datetime'].dt.strftime('%d-%b-%Y')\n",
    "\n",
    "    # Group by formatted date and count unique filenames\n",
    "    result = df_valid.groupby('date_str')['filename'].nunique().to_dict()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "answer3 = datewise_unique_filename_counts()\n",
    "print(\"Answer 3:\")\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da36a",
   "metadata": {},
   "source": [
    "### Q4: Number of 404 response codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0671865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4:\n",
      "23517\n"
     ]
    }
   ],
   "source": [
    "def count_404_errors() -> int:\n",
    "    \"\"\"\n",
    "    Q4: Number of 404 response codes.\n",
    "\n",
    "    Objective:\n",
    "        Count how many times the HTTP 404 Not Found status appears in the logs.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of 404 errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to count 404 errors\n",
    "\n",
    "    return (df['status'] == 404).sum()\n",
    "\n",
    "\n",
    "answer4 = count_404_errors()\n",
    "print(\"Answer 4:\")\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73928d2",
   "metadata": {},
   "source": [
    "### Q5: Top 15 filenames with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "358f0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5:\n",
      "[('index.html', 4694), ('4115.html', 902), ('1611.html', 649), ('5698.xbm', 585), ('710.txt', 408), ('2002.html', 258), ('2177.gif', 193), ('10695.ps', 161), ('6555.html', 153), ('487.gif', 152), ('151.html', 149), ('488.gif', 148), ('3414.gif', 148), ('40.html', 148), ('9678.gif', 142)]\n"
     ]
    }
   ],
   "source": [
    "def top_15_filenames_with_404() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q5: Top 15 filenames with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Identify which requested URLs most frequently resulted in a 404 error.\n",
    "        Return the top 15 filenames sorted by frequency.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
    "              Example: [('index.html', 200), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 15 filenames with 404\n",
    "\n",
    "    # Step 1: Filter only 404 errors\n",
    "    df_404 = df[df['status'] == 404]\n",
    "\n",
    "    # Step 2: Count frequency of each filename\n",
    "    top_15 = (\n",
    "        df_404['filename']\n",
    "        .value_counts()\n",
    "        .head(15)\n",
    "        .items()\n",
    "    )\n",
    "\n",
    "    return list(top_15)\n",
    "\n",
    "\n",
    "answer5 = top_15_filenames_with_404()\n",
    "print(\"Answer 5:\")\n",
    "print(answer5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328c88a",
   "metadata": {},
   "source": [
    "### Q6: Top 15 file extension with 404 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0aca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 6:\n",
      "[('html', 12142), ('gif', 7202), ('xbm', 824), ('ps', 754), ('jpg', 520), ('txt', 496), ('', 163), ('GIF', 135), ('htm', 107), ('cgi', 77), ('com', 45), ('Z', 41), ('dvi', 40), ('com/', 37), ('ca', 36)]\n"
     ]
    }
   ],
   "source": [
    "def top_15_ext_with_404() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q6: Top 15 file extensions with 404 responses.\n",
    "\n",
    "    Objective:\n",
    "        Find which file extensions generated the most 404 errors.\n",
    "        Return the top 15 sorted by number of 404s.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (extension, count), sorted by count in descending order.\n",
    "              Example: [('html', 45), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 15 extensions with 404\n",
    "\n",
    "    df_404 = df[df['status'] == 404]\n",
    "\n",
    "    # Step 2: Count frequency of extensions\n",
    "    top_15_exts = (\n",
    "        df_404['extension']\n",
    "        .value_counts()\n",
    "        .head(15)\n",
    "        .items()\n",
    "    )\n",
    "\n",
    "    return list(top_15_exts)\n",
    "\n",
    "\n",
    "answer6 = top_15_ext_with_404()\n",
    "print(\"Answer 6:\")\n",
    "print(answer6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52c8ba",
   "metadata": {},
   "source": [
    "### Q7: Total bandwidth transferred per day for the month of July 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45f52d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7:\n",
      "{'01-Jul-1995': 11349799, '02-Jul-1995': 8656918, '03-Jul-1995': 13596612, '04-Jul-1995': 26573988, '05-Jul-1995': 19541225, '06-Jul-1995': 19755015, '07-Jul-1995': 9427822, '08-Jul-1995': 5403491, '09-Jul-1995': 4660556, '10-Jul-1995': 14917754, '11-Jul-1995': 22507207, '12-Jul-1995': 17367065, '13-Jul-1995': 15989234, '14-Jul-1995': 19186430, '15-Jul-1995': 15773233, '16-Jul-1995': 9016378, '17-Jul-1995': 19601338, '18-Jul-1995': 17099761, '19-Jul-1995': 17851725, '20-Jul-1995': 20752623, '21-Jul-1995': 25491617, '22-Jul-1995': 8136259, '23-Jul-1995': 9593870, '24-Jul-1995': 22308265, '25-Jul-1995': 24561635, '26-Jul-1995': 24995540, '27-Jul-1995': 25969995, '28-Jul-1995': 36460693, '29-Jul-1995': 11700624, '30-Jul-1995': 23189598, '31-Jul-1995': 30730715}\n"
     ]
    }
   ],
   "source": [
    "def total_bandwidth_per_day() -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Q7: Total bandwidth transferred per day for the month of July 1995.\n",
    "\n",
    "    Objective:\n",
    "        Sum the number of bytes transferred per day.\n",
    "        Skip entries where the byte field is missing or '-'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each date to total bytes transferred.\n",
    "              Example: {'01-Jul-1995': 123456789, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to compute total bandwidth per day\n",
    "\n",
    "    # Ensure datetime is in correct format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    # Drop invalid datetimes\n",
    "    df_valid = df.dropna(subset=['datetime'])\n",
    "\n",
    "    # Create a copy of the filtered July 1995 data to avoid SettingWithCopyWarning\n",
    "    df_july95 = df_valid[\n",
    "        (df_valid['datetime'].dt.year == 1995) &\n",
    "        (df_valid['datetime'].dt.month == 7)\n",
    "    ].copy()\n",
    "\n",
    "    # Add formatted date string\n",
    "    df_july95['date_str'] = df_july95['datetime'].dt.strftime('%d-%b-%Y')\n",
    "\n",
    "    # Group by date and sum bytes\n",
    "    bandwidth_per_day = df_july95.groupby('date_str')['bytes'].sum().to_dict()\n",
    "\n",
    "    return bandwidth_per_day\n",
    "\n",
    "\n",
    "answer7 = total_bandwidth_per_day()\n",
    "print(\"Answer 7:\")\n",
    "print(answer7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc00908",
   "metadata": {},
   "source": [
    "### Q8: Hourly request distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a77f3e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 8:\n",
      "{0: 11598, 1: 9913, 2: 9403, 3: 8147, 4: 7820, 5: 8283, 6: 9798, 7: 11930, 8: 17351, 9: 21681, 10: 25713, 11: 28665, 12: 26845, 13: 30089, 14: 29792, 15: 28149, 16: 28286, 17: 23312, 18: 17862, 19: 17325, 20: 17488, 21: 15965, 22: 14587, 23: 13613}\n"
     ]
    }
   ],
   "source": [
    "def hourly_request_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q8: Hourly request distribution.\n",
    "\n",
    "    Objective:\n",
    "        Count the number of requests made during each hour (00 to 23).\n",
    "        Useful for understanding traffic peaks.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping hour (int) to request count.\n",
    "              Example: {0: 120, 1: 90, ..., 23: 80}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic for hourly distribution\n",
    "\n",
    "    # Ensure datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    df_valid = df.dropna(subset=['datetime'])\n",
    "\n",
    "    # Extract hour and count occurrences\n",
    "    hourly_counts = (\n",
    "        df_valid['datetime'].dt.hour\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    return hourly_counts\n",
    "\n",
    "\n",
    "answer8 = hourly_request_distribution()\n",
    "print(\"Answer 8:\")\n",
    "print(answer8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b7083",
   "metadata": {},
   "source": [
    "### Q9: Top 10 most requested filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91168ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 9:\n",
      "[('index.html', 139528), ('3.gif', 24006), ('2.gif', 23595), ('4.gif', 8018), ('244.gif', 5148), ('5.html', 5010), ('4097.gif', 4874), ('8870.jpg', 4492), ('6733.gif', 4278), ('8472.gif', 3843)]\n"
     ]
    }
   ],
   "source": [
    "def top_10_most_requested_filenames() -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Q9: Top 10 most requested filenames.\n",
    "\n",
    "    Objective:\n",
    "        Identify the most commonly requested URLs (irrespective of status code).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (filename, count), sorted by count in descending order.\n",
    "                Example: [('index.html', 500), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic to find top 10 most requested filenames\n",
    "\n",
    "    top_10 = (\n",
    "        df['filename']\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .items()\n",
    "    )\n",
    "    return list(top_10)\n",
    "\n",
    "\n",
    "answer9 = top_10_most_requested_filenames()\n",
    "print(\"Answer 9:\")\n",
    "print(answer9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb4778",
   "metadata": {},
   "source": [
    "### Q10: HTTP response code distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd4453ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 10:\n",
      "{200: 568345, 302: 30295, 304: 97792, 400: 15, 401: 46, 403: 4741, 404: 23517, 500: 42, 501: 43}\n"
     ]
    }
   ],
   "source": [
    "def response_code_distribution() -> dict[int, int]:\n",
    "    \"\"\"\n",
    "    Q10: HTTP response code distribution.\n",
    "\n",
    "    Objective:\n",
    "        Count how often each HTTP status code appears in the logs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping HTTP status codes (as int) to their frequency.\n",
    "              Example: {200: 150000, 404: 3000}\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement logic for response code counts\n",
    "\n",
    "    return df['status'].value_counts().sort_index().to_dict()\n",
    "\n",
    "\n",
    "answer10 = response_code_distribution()\n",
    "print(\"Answer 10:\")\n",
    "print(answer10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
